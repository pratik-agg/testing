{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-01-30 07:53:36'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "from pyspark.sql import Row, functions as F\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## path and repartition variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To read all the files in the working directory\n",
    "##for meta campaign data and travelers data\n",
    "\n",
    "import os\n",
    "files_path_dir = \"/Users/curiouspratik/Work/data\"\n",
    "list_files = os.listdir(files_path_dir)\n",
    "\n",
    "rep=1\n",
    "##content file path\n",
    "content_files_path = \"/Users/curiouspratik/Work/content_data/\"\n",
    "\n",
    "rep = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing meta campaign raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['campaign_definition.tab', 'module_definition.tab', 'module_variable_definition.tab', 'segment_definition.tab', 'segment_module.tab', 'template_definition.tab', 'traveler.tab']\n"
     ]
    }
   ],
   "source": [
    "#Filtering based on the required campaign files \n",
    "tab_file_path = []\n",
    "required_campaign_files = ['campaign_definition.tab', 'template_definition.tab','segment_module.tab',\n",
    "                          'module_variable_definition.tab','segment_definition.tab','module_definition.tab','traveler.tab']\n",
    "\n",
    "#we can remove the required_campaign_files , then it would read all the files in the directory\n",
    "for files in list_files:\n",
    "    if (files.endswith('.tab') and files in required_campaign_files):\n",
    "         tab_file_path.append(files)       \n",
    "print(tab_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-26103259316d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-26103259316d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    spark-submit --packages com.databricks:spark-csv_2.10:1.2.0 your_script.py\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "spark-submit --packages com.databricks:spark-csv_2.10:1.2.0 your_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o23.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.databricks.spark.csv. Please find packages at http://spark-packages.org\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.lookupDataSource(ResolvedDataSource.scala:77)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:102)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:119)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:109)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassNotFoundException: com.databricks.spark.csv.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4$$anonfun$apply$1.apply(ResolvedDataSource.scala:62)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4$$anonfun$apply$1.apply(ResolvedDataSource.scala:62)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4.apply(ResolvedDataSource.scala:62)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4.apply(ResolvedDataSource.scala:62)\n\tat scala.util.Try.orElse(Try.scala:82)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.lookupDataSource(ResolvedDataSource.scala:62)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-36897ec39b69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_framename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'df'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     globals()[data_framename] = (sqlContext.read.format(\"com.databricks.spark.csv\").option(\"delimiter\",\"\\t\")\n\u001b[0;32m----> 5\u001b[0;31m                                 .option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(files_path_dir+'/'+file_name))\n\u001b[0m",
      "\u001b[0;32m/Users/curiouspratik/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    135\u001b[0m                     self._jreader.load(self._sqlContext._sc._jvm.PythonUtils.toSeq(path)))\n\u001b[1;32m    136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/curiouspratik/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/curiouspratik/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/curiouspratik/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o23.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.databricks.spark.csv. Please find packages at http://spark-packages.org\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.lookupDataSource(ResolvedDataSource.scala:77)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:102)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:119)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:109)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassNotFoundException: com.databricks.spark.csv.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4$$anonfun$apply$1.apply(ResolvedDataSource.scala:62)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4$$anonfun$apply$1.apply(ResolvedDataSource.scala:62)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4.apply(ResolvedDataSource.scala:62)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$$anonfun$4.apply(ResolvedDataSource.scala:62)\n\tat scala.util.Try.orElse(Try.scala:82)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.lookupDataSource(ResolvedDataSource.scala:62)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "#this code would create all the required dataframes dynamically \n",
    "for file_name in tab_file_path:\n",
    "    data_framename = 'df'+(''.join([i.title() for i in file_name.split('.')[0].split('_')]))\n",
    "    globals()[data_framename] = (sqlContext.read.format(\"com.databricks.spark.csv\").option(\"delimiter\",\"\\t\")\n",
    "                                .option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(files_path_dir+'/'+file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#getting all the content files required\n",
    "\n",
    "def file_name(row):\n",
    "    join_key = row\n",
    "    if( len(join_key.split('|')) > 1):\n",
    "        return join_key.split('|')[1].split(';')[0].split('.')[0]\n",
    "    \n",
    "file_name_udf = udf(file_name,StringType())\n",
    "\n",
    "file_names = (dfModuleVariableDefinition.select(\"var_source\").distinct()\n",
    "                .withColumn(\"file_name\",file_name_udf('var_source')))\n",
    "\n",
    "ls_files = file_names.select(\"file_name\").filter(file_names.file_name.isNotNull()).distinct().flatMap(lambda x:x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading Content Files Dynamically\n",
    "\n",
    "frames_dict = {}\n",
    "\n",
    "for file_name in ls_files:\n",
    "    path = content_files_path + file_name + \".tab\"\n",
    "    key = file_name\n",
    "    frames_dict[key] = (sc.broadcast(sqlContext.read.format(\"com.databricks.spark.csv\").option(\"delimiter\",\"\\t\")\n",
    "                                .option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(path).toPandas()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#correcting the value of locale\n",
    "\n",
    "def replace(x):\n",
    "    return x.replace('_','-')\n",
    "\n",
    "\n",
    "udf_replace = udf(replace,StringType())\n",
    "dfTraveler = dfTraveler.withColumn(\"locale\",udf_replace(dfTraveler.locale))\n",
    "dfModuleDefinition = dfModuleDefinition.withColumn(\"locale\",udf_replace(dfModuleDefinition.locale))\n",
    "dfCampaignDefinition = dfCampaignDefinition.withColumn(\"locale\",udf_replace(dfCampaignDefinition.locale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## changing column name of the loyalty column as present in segment criteria\n",
    "\n",
    "dfTraveler = dfTraveler.withColumnRenamed(\"loyalty_tier_name\",\"loyalty_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemplateDefinition File OK!!\n"
     ]
    }
   ],
   "source": [
    "#Checking dfTemplateDefinition File\n",
    "\n",
    "if (dfTemplateDefinition\n",
    "    .select(\"template_id\",\"segment_module_map_id\",\"module_priority_in_slot\").distinct().count() == dfTemplateDefinition.count()):\n",
    "    print (\"TemplateDefinition File OK!!\")\n",
    "else:\n",
    "    print (\"Appending TemplateDefinition!!\")\n",
    "    dfTemplateDefinition = (dfTemplateDefinition\n",
    "                            .groupBy(\"template_id\",\"segment_module_map_id\",\"module_priority_in_slot\")\n",
    "                            .agg({\"slot_position\":\"min\",\"placement_type\":\"first\"})\n",
    "                            .withColumnRenamed(\"first(placement_type)\",\"placement_type\")\n",
    "                            .withColumnRenamed(\"min(slot_position)\",\"slot_position\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dfTemplateDefinition.toPandas().to_csv(\"/home/affine/Downloads/Untitled Folder/unit_test_files/dfTemplateDefinition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## populating values in all rows for var source column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def coalesceCol(x,y):\n",
    "    if (x != \"\"):\n",
    "        return x\n",
    "    else:\n",
    "        return y\n",
    "udfcoalesceCol = udf(coalesceCol, StringType())\n",
    "\n",
    "module_var_def = dfModuleVariableDefinition\n",
    "\n",
    "df2 = (module_var_def.select(\"module_id\",\"source_connection\",\"var_source\")\n",
    "       .filter(\"var_source != ''\")\n",
    "       .withColumnRenamed(\"var_source\",\"var_source1\"))   \n",
    "\n",
    "df3 = module_var_def.join(df2,[\"module_id\",\"source_connection\"],'left')\n",
    "\n",
    "dfModuleVariableDefinition = (df3\n",
    "                        .withColumn(\"var_source_final\", udfcoalesceCol(df3.var_source,df3.var_source1))\n",
    "                        .distinct()\n",
    "                        .drop(\"var_source\").drop(\"var_source1\")\n",
    "                        .withColumnRenamed(\"var_source_final\",\"var_source\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dfModuleVariableDefinition.toPandas().to_csv(\"/home/affine/Downloads/Untitled Folder/unit_test_files/dfModuleVariableDefinition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating meta campaign data by combining all the raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day_of_Week = 1   ###datetime.datetime.today().weekday() keep need to change it\n",
    "dfMetaCampaignData1 = dfCampaignDefinition[(dfCampaignDefinition['dayofweek'] == day_of_Week+1)].orderBy(\"priority\")\n",
    "\n",
    "\n",
    "dfMetaCampaignData2 = (dfMetaCampaignData1.join(dfTemplateDefinition,'template_id','inner')\n",
    "                                              .join(dfSegmentModule,'segment_module_map_id','inner')\n",
    "                                              .join(dfModuleDefinition,['locale','module_type_id','tpid','eapid','placement_type','channel'],'inner')                                              \n",
    "                                              .join(dfSegmentDefinition,[\"tpid\",\"eapid\",\"segment_type_id\"],'inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfMetaCampaignData_31 = (dfMetaCampaignData2.select([c for c in dfMetaCampaignData2.columns if c not in \n",
    "                                                {\"campaign_type_id\",\"dayofweek\",\"program_type\",\n",
    "                                                 \"derived_module_id\",\"context\",\"language\",\"lob_intent\",\"status\"}])\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dfMetaCampaignData_31.toPandas().to_csv(\"/root/Expedia_DE/Data/projectalpha/data/module_type_test_files/dfMetaCampaignData_31.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correcting name of the column email_address\n",
    "\n",
    "dfTravelers = (dfTraveler.withColumnRenamed(\"\\ufeffemail_address\",\"email_address\").cache())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populating segment type id in travelers data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfMetaCampaign = (dfMetaCampaignData_31\n",
    "                  .select(\"tpid\",\"eapid\",\"locale\",\"segment_type_id\",\"segment_criteria\",\"campaign_id\",\"priority\")\n",
    "                  .distinct()\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## removing the segment criteria related to reward_redeem as this column is not present in travaler data\n",
    "\n",
    "segment_criteria = (dfSegmentDefinition.filter(\"segment_type_name != 'reward_redeem'\")\n",
    "                    .select(\"segment_criteria\").distinct()\n",
    "                    .flatMap(lambda x:x)\n",
    "                    .collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = 1\n",
    "\n",
    "for i in segment_criteria:    \n",
    "    criteria = (i.replace('[','').replace(']',''))    \n",
    "    tmp = (dfTravelers.withColumn(\"segment_criteria\",lit(i)).filter(criteria))\n",
    "    \n",
    "    if counter == 1:\n",
    "        traveler = tmp\n",
    "        counter+=1\n",
    "    else:\n",
    "        traveler = traveler.unionAll(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##changed the join from left to inner\n",
    "dfTraveler_MetaCampaign = (traveler.join(dfMetaCampaign,[\"segment_criteria\",\"tpid\",\"eapid\",\"locale\"],'inner')\n",
    "                            .distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"email_address\").orderBy(\"priority\")\n",
    "\n",
    "traveler_RU = (dfTraveler_MetaCampaign.withColumn(\"index\", dense_rank().over(w))\n",
    "                        .filter(\"index = 1\").drop(\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traveler_RU_multi_row_treat = (traveler_RU\n",
    "                             .groupBy('email_address')\n",
    "                             .agg({'segment_type_id':'max','campaign_id':'first'})\n",
    "                             .withColumnRenamed('max(segment_type_id)','segment_type_id')\n",
    "                             .withColumnRenamed('first(campaign_id)','campaign_id')\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df_for_alpha = (traveler_RU_multi_row_treat\n",
    "                              .join(traveler_RU,['email_address','campaign_id','segment_type_id'],'inner')\n",
    "                              .repartition(rep)\n",
    "                              .cache()\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final_df_for_alpha.toPandas().to_csv(\"/root/Expedia_DE/Data/projectalpha/data/module_type_test_files/final_df_for_alpha.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating module version mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dict_map(module_type_id,num_versions):\n",
    "    import math\n",
    "    \n",
    "    seed = datetime.date.today().isocalendar()[1] \n",
    "    split = math.ceil((99/num_versions))\n",
    "    \n",
    "    rank_dict = {}\n",
    "    for i in range(num_versions):\n",
    "        rank = ((i) + seed%num_versions)%num_versions+1\n",
    "        rank_dict[rank] = i+1\n",
    "\n",
    "    range_dict = {}\n",
    "    j = 1\n",
    "    for key in rank_dict:\n",
    "        range_dict[rank_dict[key]] = str(j)+'#'+str(j+split-1)\n",
    "        j+= split\n",
    "    \n",
    "    return range_dict\n",
    "\n",
    "dict_map_udf = udf(dict_map,MapType(IntegerType(),StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_module = (dfMetaCampaignData_31\n",
    "             .groupBy('module_type_id')\n",
    "             .agg(countDistinct('version'))\n",
    "             .withColumnRenamed('count(version)','num_versions')\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_grp = ['campaign_id','slot_position','module_type_id']\n",
    "\n",
    "dfMetaCampaignData_4 = (dfMetaCampaignData_31\n",
    "                        .select(col_grp)\n",
    "                        .join(df_module,'module_type_id','inner')\n",
    "                        .distinct()\n",
    "                        .withColumn('version_map',dict_map_udf('module_type_id','num_versions'))\n",
    "                        .drop('num_versions').drop('travelers')\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dfMetaCampaignData_4.toPandas().to_csv(\"dfMetaCampaignData_4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining module_var_definition with meta campaign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Joining with dfModuleDefinition to populate Module Priority and with dfSegmentsDefinition to fetch Segment Criteria\n",
    "dfMetaCampaignData_VarDef = (dfMetaCampaignData_31\n",
    "                         .join(dfModuleVariableDefinition,\"module_id\",'inner')\n",
    "                         .join(dfMetaCampaignData_4,col_grp,'inner')\n",
    "                         .drop(\"locale\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#module_content_data_final.toPandas().to_csv(\"/root/Expedia_DE/Data/projectalpha/data/module_type_test_files/module_content_data_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#selecting the required columns for module id allocation\n",
    "col_list = ['campaign_id','slot_position','var_structure','var_source','var_position','module_id','module_type_id'\n",
    "           ,'module_priority','segment_type_id','version','version_map','module_priority_in_slot']\n",
    "\n",
    "\n",
    "dict_cpgn = sc.broadcast(dfMetaCampaignData_VarDef\n",
    "                        .select(col_list).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slot_position_map = (dfMetaCampaignData_VarDef\n",
    "                     .groupBy('slot_position')\n",
    "                     .agg({'var_position':'max'})\n",
    "                     .withColumnRenamed('max(var_position)','var_position')\n",
    "                     .rdd\n",
    "                     .collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slots = dfTemplateDefinition.select('slot_position').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_slot = final_df_for_alpha.withColumn('number_slots',lit(slots)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fun(campaign_id,test_keys,segment_type_id,number_slots):\n",
    "    \n",
    "    import pandas as pd\n",
    "    num_slots = number_slots\n",
    "    map_dict1 = {}\n",
    "    \n",
    "    for i in range(1,(num_slots+1)):\n",
    "        slot_position = i\n",
    "        cpgn_sub = dict_cpgn.value[(dict_cpgn.value.campaign_id == campaign_id) &\n",
    "                                  (dict_cpgn.value.slot_position == slot_position) ]\n",
    "\n",
    "        cpgn_sub_final = cpgn_sub[(cpgn_sub['segment_type_id'] == segment_type_id)]\n",
    "\n",
    "        if len(cpgn_sub_final) == 0:\n",
    "            cpgn_sub_final = cpgn_sub[(cpgn_sub['segment_type_id'] == 1)]\n",
    "\n",
    "\n",
    "        test = pd.DataFrame()\n",
    "\n",
    "        module_types = list(cpgn_sub_final.module_type_id.unique())\n",
    "\n",
    "        for module in module_types:\n",
    "\n",
    "            version_sel = cpgn_sub_final[(cpgn_sub_final.module_type_id == module)].iloc[0].version_map\n",
    "\n",
    "            for key in version_sel:\n",
    "\n",
    "                ll = int(version_sel[key].split('#')[0])\n",
    "                ul = int(version_sel[key].split('#')[1])\n",
    "\n",
    "                if (test_keys >=ll) & (test_keys <=ul):\n",
    "                    test = test.append(cpgn_sub_final[(cpgn_sub_final.version==key) & (cpgn_sub_final.module_type_id == module)])\n",
    "\n",
    "\n",
    "        col_req = ['module_type_id','module_priority']\n",
    "\n",
    "        test.index = (test[col_req]\n",
    "                      .astype(str)\n",
    "                      .apply(lambda x : '#'.join(x), axis=1))\n",
    "\n",
    "        test['module_id'] = test['module_id'].astype('str')\n",
    "\n",
    "        final_dict = test[['module_id']].to_dict()['module_id'] \n",
    "        map_dict1[str(i)] = final_dict\n",
    "    \n",
    "    return map_dict1\n",
    "\n",
    "fun_udf = udf(fun,MapType(StringType(),MapType(StringType(),StringType())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_slot_fun = df_slot.withColumn('map_dict1',fun_udf('campaign_id','test_keys','segment_type_id','number_slots'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def module_info(map_dict1):\n",
    "    map_dict = {}\n",
    "    for slot in map_dict1:\n",
    "        for key in map_dict1[slot]:\n",
    "            module_id = map_dict1[slot][key]\n",
    "            test = dict_cpgn.value[dict_cpgn.value.module_id == int(module_id)][['var_source','var_position','var_structure']]\n",
    "            col_req = ['var_source','var_position']\n",
    "            test.index = (test[col_req]\n",
    "                          .astype(str)\n",
    "                          .apply(lambda x : '#'.join(x), axis=1))\n",
    "            test_dict = test[['var_structure']].to_dict()['var_structure']\n",
    "            map_dict[str(module_id)] = test_dict\n",
    "\n",
    "    return map_dict\n",
    "    \n",
    "module_info_udf = udf(module_info,MapType(StringType(),MapType(StringType(),StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_slot_fun_module = df_slot_fun.withColumn('map_dict',module_info_udf('map_dict1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## populating the value in columns created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def content_map(row):\n",
    "    \n",
    "    import re\n",
    "    dict_req = {'email_address':row['email_address']}\n",
    "    number_slots = row['number_slots']\n",
    "    for slot_position in range(1,number_slots+1):\n",
    "    \n",
    "        map_dict1 = row['map_dict1'][str(slot_position)]\n",
    "        slot_position = str(slot_position)\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for key1 in  map_dict1:\n",
    "\n",
    "            map_dict = row['map_dict'][map_dict1[key1]]\n",
    "\n",
    "            var_positions = slot_position_map[int(slot_position)]\n",
    "\n",
    "            x_dict = {('S'+slot_position+'_P'+str(v)):'' for v in range(1,var_positions+1)}\n",
    "\n",
    "            x_dict['S'+slot_position+'_module_priority'] = int(key1.split('#')[1])\n",
    "\n",
    "            for key in map_dict:\n",
    "\n",
    "                join_key =  key.split('#')[0]\n",
    "\n",
    "                slot_var_pos = 'S'+slot_position+'_P'+str(key.split('#')[1]) #Assigning the Slot_Pos Name\n",
    "\n",
    "                if( len(join_key.split('|')) > 1):\n",
    "                    file_name =  join_key.split('|')[1].split(';')[0].split('.')[0]\n",
    "                    left_keys = [col.split('.')[1] for col in join_key.split('|')[0].split(';')]\n",
    "                    right_keys = [col.split('.')[1] for col in join_key.split('|')[1].split(';')]\n",
    "\n",
    "                else :\n",
    "                    file_name =  'NA'\n",
    "                    left_keys = 'NA'\n",
    "                    right_keys = 'NA'\n",
    "\n",
    "\n",
    "                if (len(map_dict[key]) == 0):\n",
    "                    var_value = \"\"\n",
    "\n",
    "                elif(len(map_dict[key].split('%%')) == 1):\n",
    "                     var_value = map_dict[key]\n",
    "\n",
    "                else : \n",
    "                    z = [m.start() for m in re.finditer('%%', map_dict[key])]\n",
    "                    var_value = map_dict[key]\n",
    "\n",
    "                    j = 0\n",
    "                    while (j <= len(z)-2):\n",
    "                        temp = map_dict[key][z[j]+2:z[j+1]]\n",
    "                        df_name = temp.split('.')[0]\n",
    "\n",
    "\n",
    "                        if (df_name == 'traveler'):\n",
    "\n",
    "                            trav_attri = row[temp.split('.')[1]]\n",
    "\n",
    "                            if trav_attri == '':\n",
    "                                trav_attri = \"value_not_found\"\n",
    "\n",
    "                            var_value = var_value.replace(temp,trav_attri)\n",
    "\n",
    "                        else :\n",
    "\n",
    "                            req_content = frames_dict[df_name].value\n",
    "\n",
    "                            for i in range(len(right_keys)):\n",
    "                                req_content = req_content[req_content[right_keys[i]]==row[left_keys[i]]]\n",
    "\n",
    "                            if len(req_content)==0:\n",
    "                                x_dict['S'+slot_position+'_module_priority'] = 9999\n",
    "\n",
    "                            content_ls = list(req_content[temp.split('.')[1]])\n",
    "\n",
    "                            if len(content_ls) > 0:\n",
    "                                value_re = content_ls[0]\n",
    "                                if value_re == '':\n",
    "                                    value_re = \"value_not_found\"\n",
    "                            else : \n",
    "                                value_re = \"value_not_found\"\n",
    "                                x_dict['S'+slot_position+'_module_priority'] = 9999\n",
    "\n",
    "                            var_value = var_value.replace(temp,str(value_re))\n",
    "\n",
    "                        j+=2\n",
    "\n",
    "                x = var_value.replace('%%','')\n",
    "\n",
    "                ttl_options = len(x.split(\"|\"))\n",
    "                seed = datetime.date.today().isocalendar()[1] \n",
    "\n",
    "                if(ttl_options > 2):\n",
    "                    att = int((row['test_keys']+ seed%(ttl_options-1))%(ttl_options-1)) ####\n",
    "                elif (ttl_options == 2):\n",
    "                    att = int((row['test_keys']+ seed%(ttl_options))%(ttl_options)) ####\n",
    "                else:\n",
    "                    att = 0\n",
    "\n",
    "                if ((x.lower().find('value_not_found') > 0)):\n",
    "                    x_dict[slot_var_pos] = x.split('|')[0].replace('value_not_found','')\n",
    "\n",
    "                else:\n",
    "                    if (ttl_options<=1):\n",
    "                        x_dict[slot_var_pos] = x.split('|')[0].replace('value_not_found','')\n",
    "                    elif (ttl_options==2):\n",
    "                        list_value = x.split('|')\n",
    "                        x_dict[slot_var_pos]=(list_value[att]).replace('value_not_found','')\n",
    "                    else :\n",
    "                        list_value = x.split('|')[1:]\n",
    "                        x_dict[slot_var_pos]=(list_value[att]).replace('value_not_found','')\n",
    "\n",
    "\n",
    "                x_dict['S'+slot_position+'_module_id'] = map_dict1[key1]\n",
    "                x_dict['S'+slot_position+'_module_type_id'] = str(key1.split('#')[0])\n",
    "\n",
    "\n",
    "            temp = pd.DataFrame(x_dict,index=[x_dict['S'+slot_position+'_module_priority']])\n",
    "\n",
    "            df = df.append(temp)\n",
    "\n",
    "        final_df = df.loc[[df['S'+slot_position+'_module_priority'].idxmin()]]      \n",
    "        col_name = list(final_df['S'+slot_position+'_module_priority'])[0]\n",
    "\n",
    "        mod_id = final_df['S'+slot_position+'_module_id'].iloc[0]\n",
    "\n",
    "        for prior in map_dict1:\n",
    "            if map_dict1[prior] == mod_id:\n",
    "                module_prior = prior.split('#')[1]\n",
    "\n",
    "        final_df['S'+slot_position+'_module_priority'] =  module_prior\n",
    "        final_dict = final_df.transpose().to_dict()[col_name]\n",
    "\n",
    "        dict_req.update(final_dict)\n",
    "    \n",
    "    return Row(**dict_req)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_data = df_slot_fun_module.rdd.map(lambda x : content_map(x)).toDF().repartition(rep,'email_address').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_result = final_df_for_alpha.join(final_data,'email_address','inner').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final_result.toPandas().to_csv('final_result_v5.8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
